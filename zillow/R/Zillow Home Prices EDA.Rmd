---
title: "Kaggle - Zillow Home Prices EDA"
author: "Lilian Cheung"
date: "March 18, 2018"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(RCurl)
library(data.table)
library(caret)
library(corrplot)
library(xts)
library(xgboost)
library(pls)
library(Matrix)
library(vcd)
```


```{r read in datasets}
runthis <- FALSE

if(runthis==TRUE) {
path_properties_2016 <- "C:/Lilian/Documents/Technical Training/R Projects/Kaggle/Zillow Home Prices/properties_2016/properties_2016.csv"

path_properties_2017 <- "C:/Lilian/Documents/Technical Training/R Projects/Kaggle/Zillow Home Prices/properties_2017/properties_2017.csv"

path_train_2016 <- "C:/Lilian/Documents/Technical Training/R Projects/Kaggle/Zillow Home Prices/train_2016_v2/train_2016_v2.csv"

path_train_2017 <- "C:/Lilian/Documents/Technical Training/R Projects/Kaggle/Zillow Home Prices/train_2017/train_2017.csv"

## Training Data - 2016 (properties and logerrors of sales)
  properties_2016 <- fread( file=path_properties_2016, header=TRUE )
  train_2016 <- fread(file=path_train_2016, header=TRUE)

## Training Data - 2017 (properties and logerrors of sales)
  properties_2017 <- fread( file=path_properties_2017, header=TRUE )
  train_2017 <- fread(file=path_train_2017, header=TRUE)
}

```


```{r Create Training Dataset by merging properties and logerrors of sales}
runthis <- FALSE

if (runthis==TRUE){
# Merge training data from 2016
  training_2016 <- merge( properties_2016, train_2016, by="parcelid" )

# Merge training data from 2017
  training_2017 <- merge( properties_2017, train_2017, by="parcelid")
  
# Combine 2016 and 2017 data into a dataframe
  training_all <- rbind(training_2016, training_2017)

# Save locally
  save_path <- "C:/Lilian/Documents/Technical Training/R Projects/Kaggle/Zillow Home Prices/training_all.csv"
  write.csv( training_all, file=save_path, row.names=FALSE)
}

```


```{r}
# Read in dataset
  training_all_path <- "C:/Lilian/Documents/Technical Training/R Projects/Kaggle/Zillow Home Prices/training_all.csv"
  training_all <- read.csv( file=training_all_path, header=TRUE)

## Sample some percent of dataset for EDA
  set.seed(123)
  percent <- 0.10
  training_sample <- sample_n( training_all, percent*nrow(training_all), replace=FALSE )
  
  ## remaining data 
  # remaining_data <- dplyr::filter( training_all, !parcelid %in% training_sample[["parcelid"]] )

## Save locally
  save_path <- "C:/Lilian/Documents/Technical Training/R Projects/Kaggle/Zillow Home Prices/training_sample.csv"
  write.csv( training_sample, file=save_path, row.names=FALSE)
  
```


```{r}
## Read in training sample
  read_path <- "C:/Lilian/Documents/Technical Training/R Projects/Kaggle/Zillow Home Prices/training_sample.csv"
  training_sample <- read.csv( file=read_path, header=TRUE)
  training_sample <- mutate(training_sample, transactiondate = as.Date(transactiondate))
  
## Reorder training sample by date
  training_sample <- arrange( training_sample, transactiondate )
  
```


```{r}
## Explore transaction counts in sample

# Number of transactions per day
  counts_transactions <- count( training_sample, transactiondate )
  
  n_transactions_plot <- ggplot( counts_transactions, aes(x = transactiondate, y = n) ) +
                          geom_line( color = "skyblue" ) + 
                          xlab("Transaction Date") + 
                          ylab("Number of Transactions") + 
                          ggtitle("Number of Transactions Per Day")
  
  print( n_transactions_plot )

# Number of transactions per month in our dataset (note that October 2016 dips since part of the dataset is in Zillow's test dataset)
dt_training_sample <- data.table( training_sample )
counts_transactions_month <- dt_training_sample[ , .N, by = .(year(transactiondate), month(transactiondate)) ]

  # Graph (uses xts package)
  # Manipulate data into time series
  transactions_month <- counts_transactions_month[["N"]]
  transactions_month <- ts( counts_transactions_month[["N"]], frequency = 12, start = 2016)
  
  print( plot(as.xts(transactions_month), major.format = "%Y-%m", title="Number of Transactions per Month") )
  
  par(mar=c(5.1,4.1,4.1,2.1))
  acf(counts_transactions$n, lag.max = 30)


```

```{r, warnings=FALSE}
## Exploring the continuous predictors

## Convert variable types to factor as necessary
  to_factor <- c( "airconditioningtypeid", "architecturalstyletypeid",
        "buildingqualitytypeid", "buildingclasstypeid", "decktypeid",
        "fips", "heatingorsystemtypeid",
        "parcelid", "poolcnt", "pooltypeid10", "pooltypeid2", "pooltypeid7",
        "propertylandusetypeid", "rawcensustractandblock", 
        "censustractandblock",
        "regionidcounty", "regionidcity", "regionidzip", 
        "regionidneighborhood", "storytypeid", "typeconstructiontypeid",
        "assessmentyear", "taxdelinquencyyear")
  training_sample[ to_factor ] <- lapply( training_sample[to_factor], factor)  
  
  
## Examine the percentage of missing values per variable
  
  # Define a function for finding the percentage of missing values in a column
      pct_missing_in_column <- function( column ){
        
        total_obs <- length( column )
        pct_missing <- sum( is.na(column)==TRUE )/total_obs
        
        pct_missing
        
      }
      
      pct_missing <- map_dbl( training_sample, pct_missing_in_column )
    
    # Which variables are missing under 75% of data?
      missing_ok_vars <- pct_missing[ pct_missing <= 0.75 ]
    
    # Keep only the variables missing less than 75% of data
      training_sample <- training_sample[ names(missing_ok_vars) ]
  
      
## Examine continuous variables
  num_columns <- sapply(training_sample, is.numeric)
  cts_training_sample <- training_sample[ num_columns ]
  
      
  # Correlogram
  cor_matrix <- cor( cts_training_sample, use='pairwise.complete.obs' )
  corrplot(cor_matrix, type='lower')
  
  
```


```{r}
## Explore logerror, the response variable

  logerror_histogram <- ggplot( training_sample, aes(x = logerror) ) +
                        geom_histogram( bins = 20)
  logerror_histogram

## Plot of logerror per day
  mean_logerror_daily <- dt_training_sample[, mean(logerror), by = transactiondate]
    names(mean_logerror_daily) <- c("transactiondate", "mean_daily_logerror")
  
  daily_mean_logerror_plot <- ggplot( mean_logerror_daily, aes(x = transactiondate, y = mean_daily_logerror) ) +
                          geom_line( color = "darkred" ) + 
                          xlab("Transaction Date") + 
                          ylab("Mean Logerror") + 
                          ggtitle("Mean Logerror Per Day")
  
  daily_mean_logerror_plot
  
## Does average logerror per day exhibit autocorrelation? No.
  acf( mean_logerror_daily[["mean_daily_logerror"]] )
  
## As expected, number of daily transactions does not have a high correlation with mean daily logerrors
  cor(x=mean_logerror_daily$mean_daily_logerror, y=counts_transactions$n)
  # 0.05

```


```{r}
## Correlations between high logerrors and continuous features

## Arbitrarily define a high logerror as one that meets or exceeds 0.25
  high_logerror_sample <- dplyr::filter( training_sample, abs(logerror) >= 0.25 )

## Correlations between continuous variables in the high logerror sample
  num_columns <- sapply( high_logerror_sample, is.numeric)
  cts_high_logerror_sample <- high_logerror_sample[ num_columns ]
  
  # Correlogram
    cor_matrix <- cor( cts_high_logerror_sample, use='pairwise.complete.obs' )
    corrplot(cor_matrix, type='lower')

## log(errors) = log(Z) - log(salesprice) 
# Take the exponent --> errors = Z/salesprice
  errors_sample <- mutate( training_sample, error = exp(logerror ))
  
  # Properties where Zillow overestimated the salesprice ( error > 1 ) or underestimated the salesprice ( error < 1 )
    errors_overestimated <- dplyr::filter( errors_sample, error > 1 )
    errors_underestimated <- dplyr::filter( errors_sample, error < 1 ) 
    
  # Look at continuous variables only
    num_columns <- sapply( errors_sample, is.numeric )

    cts_errors_overestimated <- errors_overestimated[ num_columns ]
    cts_errors_underestimated <- errors_underestimated[ num_columns ]
  
  
  # Correlograms
    cor_matrix <- cor( cts_errors_overestimated, use = 'pairwise.complete.obs' )
    corrplot( cor_matrix, type = 'lower' )
    
    cor_matrix <- cor( cts_errors_underestimated, use = 'pairwise.complete.obs' )
    corrplot( cor_matrix, type = 'lower' )
    
```

A preliminary look at Pearson's correlations between continuous variables does not show any strong relationships between continuous variables and logerrors, even when the dataset is subsetted by overestimated/underestimated logerrors.


```{r, warnings = FALSE}
### Principal Components Analysis - A Brief Look
## https://www.r-bloggers.com/principal-component-analysis-in-r/
## https://www.r-bloggers.com/computing-and-visualizing-pca-in-r/
## http://www.milanor.net/blog/performing-principal-components-regression-pcr-in-r/
## https://www.kaggle.com/taurothebull/xgboost-pca/code
training_sample_pca <- log( cts_training_sample + 0.0001 )

pca <- prcomp( ~bathroomcnt +  bedroomcnt +  calculatedbathnbr +  calculatedfinishedsquarefeet +  finishedsquarefeet12 +  fullbathcnt +  garagecarcnt +  garagetotalsqft + lotsizesquarefeet +  roomcnt + yearbuilt +  structuretaxvaluedollarcnt +  taxvaluedollarcnt +  landtaxvaluedollarcnt +  taxamount , data = training_sample_pca, center = TRUE, scale. = TRUE, na.action = na.omit )

print( pca ) 

plot( pca, type = "l" )

summary( pca )

pcr_mod <- pcr( logerror ~ bathroomcnt +  bedroomcnt +  calculatedbathnbr +  calculatedfinishedsquarefeet +  finishedsquarefeet12 +  fullbathcnt +  garagecarcnt +  garagetotalsqft + lotsizesquarefeet +  roomcnt + yearbuilt +  structuretaxvaluedollarcnt +  taxvaluedollarcnt +  landtaxvaluedollarcnt +  taxamount, data = cts_training_sample, scale = TRUE )

summary(pcr_mod)

validationplot( pcr_mod )
predplot( pcr_mod )

pcr_pred <- predict( pcr_mod, training_all, ncomp = 3 )
```




```{r}

## Looking at the categorical (factor) variables
# Reference: http://www.cookbook-r.com/Manipulating_data/Converting_between_data_frames_and_contingency_tables/


## Filter out factor variables from the full training dataset
  factor_vars <- sapply( training_sample, is.factor )
  factor_training_sample <- training_sample[ factor_vars ]
  factor_training_sample <- cbind( factor_training_sample, training_sample$logerror )
  names(factor_training_sample)[ncol(factor_training_sample)] <- "logerror"

## Examine the percentage of values missing in each factor variable
pct_missing_factor <- map_dbl( factor_training_sample, pct_missing_in_column )

## Filter out variables missing too many values 
  missing_ok_factors <- pct_missing_factor[ pct_missing_factor <= 0.75 ]
  factor_training_sample <- factor_training_sample[ names(missing_ok_factors) ]

## Drop some factors
  drops <- c( "parcelid", "rawcensustractandblock", "censustractandblock" )
  factor_training_sample <- factor_training_sample[ !names(factor_training_sample) %in% drops ]
  
## Is logerror at all related to these categorical variables?
mod <- lm( logerror ~ airconditioningtypeid  + buildingqualitytypeid + propertycountylandusecode  +  propertylandusetypeid + assessmentyear , data = factor_training_sample   )

summary( mod )

```

```{r}
# http://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html
# https://www.kaggle.com/zoldin/exploratory-analysis-amd-xgboost
# https://www.kaggle.com/jmbull/boost-starter-0-0648x-lb/code
# https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/


```










